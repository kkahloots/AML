{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"ds.pkl\"\n",
    "ds = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers = ['user_id', 'request_id', 'target_recipient_id']\n",
    "date_cols = ['date_user_created', 'date_request_submitted', 'date_request_received','date_request_transferred', \\\n",
    "             'first_attempt_date', 'first_success_date']\n",
    "categorical = ['addr_country_code', 'addr_city', 'recipient_country_code', 'flag_personal_business', 'payment_type', \\\n",
    "               'payment_status', 'ccy_send', 'ccy_target', 'transfer_to_self', 'sending_bank_name',  'sending_bank_country',\\\n",
    "               'payment_reference_classification', 'device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    98500\n",
       "1     1500\n",
       "Name: anomalous, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['anomalous'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We should do Over-sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.externals.joblib import Memory\n",
    "\n",
    "class dateEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Processing DateTime Data')\n",
    "        print('Encoding....')\n",
    "        retX = pd.DataFrame()\n",
    "\n",
    "        for colname, col in X.iteritems():\n",
    "            retX[colname+'_dayofweek'] = col.dt.dayofweek\n",
    "            retX[colname+'_weekday'] = col.dt.weekday\n",
    "            retX[colname+'_hour'] = col.dt.hour\n",
    "            retX[colname+'_minute'] = col.dt.minute\n",
    "            retX[colname+'_day'] = col.dt.day\n",
    "            retX[colname+'_month'] = col.dt.month\n",
    "            #retX[colname+'_year'] = col.dt.year\n",
    "    \n",
    "        return retX\n",
    "    \n",
    "def prepare_pipeline(ds):\n",
    "    \n",
    "    numeric_features = ds.select_dtypes(include=[np.float or np.float]).columns.tolist()\n",
    "    categorical_features = ds.select_dtypes(include=['object']).columns.tolist()\n",
    "    date_features = ds.select_dtypes(include=[np.datetime64]).columns.tolist()\n",
    "    \n",
    "    cachedir = mkdtemp()\n",
    "    memory = Memory(cachedir=cachedir, verbose=1)\n",
    "\n",
    "    date_transformer = Pipeline(memory=memory, steps=[('dateEncoder', dateEncoder()), ('imputer', SimpleImputer(strategy='median', verbose=1))])\n",
    "    numeric_transformer = Pipeline(memory=memory, steps=[('imputer', SimpleImputer(strategy='median', verbose=1))])\n",
    "    categorical_transformer = Pipeline(memory=memory, steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing', verbose=1)),\n",
    "                                                             ('onehot', OneHotEncoder(handle_unknown='ignore'))])    \n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                                   ('dates', date_transformer, date_features),\n",
    "                                                   ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    proc = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    \n",
    "    return proc\n",
    "\n",
    "\n",
    "class risk_estimator(IsolationForest):\n",
    "    def __init__(self, n_estimators=1000, max_samples=1000,max_features=0.6, \\\n",
    "                 contamination=0.25, n_jobs=-1, behaviour='new', bootstrap=True, verbose=1):\n",
    "\n",
    "                                    \n",
    "        super(risk_estimator, self).__init__(n_estimators=n_estimators, max_samples=max_samples, \\\n",
    "                                             max_features=max_features, bootstrap=bootstrap, \\\n",
    "                                             contamination=contamination, n_jobs=n_jobs, behaviour=behaviour, verbose=verbose)\n",
    "        self.preprocessor = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.preprocessor = prepare_pipeline(X)\n",
    "        print('Start Data pre-processing ...')\n",
    "        self.preprocessor = self.preprocessor.fit(X)\n",
    "        X = self.preprocessor.transform(X)\n",
    "        print('Size of the Dataset after processing', X.shape)\n",
    "        print('SMOTE Over-sampling the minority class ...')\n",
    "        self.resampler = SMOTE()\n",
    "        X, y = self.resampler.fit_sample(X, y)\n",
    "        print('dataset has been added into ') \n",
    "        print('Size of the Dateset after resampling', X.shape)\n",
    "        super(risk_estimator, self).fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = super(risk_estimator, self).predict(self.preprocessor.transform(X))\n",
    "        return np.array(list(map(lambda p: 1 if -1 else 0, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds[[c for c in ds.columns.tolist() if c not in identifiers+['anomalous_score', 'anomalous']]]\n",
    "y = ds['anomalous'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_holdout, y, y_holdout = train_test_split(X, y, stratify=y, test_size=0.1, shuffle=True, random_state=4891)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda34\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='C:\\\\Users\\\\KHALID~1\\\\AppData\\\\Local\\\\Temp\\\\tmpjagsx631'\", use \"location='C:\\\\Users\\\\KHALID~1\\\\AppData\\\\Local\\\\Temp\\\\tmpjagsx631'\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Data pre-processing ...\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(dateEncoder(),         date_user_created date_request_submitted date_request_received  \\\n",
      "97033 2013-09-08 01:04:00    2013-11-08 17:01:00                   NaT   \n",
      "93828 2014-10-23 12:10:00    2016-09-11 08:23:00                   NaT   \n",
      "785   2015-11-20 14:56:00    2016-07-03 16:15:00   2016-07-03 16:42:00   \n",
      "42891 2016-03-02 21:01:00    2016-11-08 03:03:00   2016-11-08 03:05:00   \n",
      "75317 2015-11-16 09:11:00    2016-05-14 06:18:00   2016-05-14 06:21:00   \n",
      "13086 2016-09-30 11:20:00    2016-10-27 05:58:00   2016-10-27 06:47:00   \n",
      "30749 2015-08-17 20:54:00    2015-08-17 21:13:00   2015-08-17 21:15:00   \n",
      "50350 2015-02-09 11:15:00    2016-03-18 10:40:00   2016-03-18 10:44:00   \n",
      "24448 2014-03-29 07:28:00    2015-..., \n",
      "None, None)\n",
      "Processing DateTime Data\n",
      "Encoding....\n",
      "________________________________________________fit_transform_one - 0.2s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
      "       strategy='constant', verbose=1), \n",
      "      addr_country_code        addr_city recipient_country_code  \\\n",
      "97033               DNK           KORSÃ˜R                     DK   \n",
      "93828               GBR         KEIGHLEY                     GB   \n",
      "785                 GBR          BRISTOL                     AT   \n",
      "42891               GBR            RIPON                     PL   \n",
      "75317               GBR          GLASGOW                     GB   \n",
      "13086               GBR        PRESTWICK                     PT   \n",
      "30749               USA       MINNETONKA                     IN   \n",
      "50350               GBR        ST ALBANS                     HU   \n",
      "24448               GBR        PLUMSTEAD                     HU   \n",
      "65440               GBR       ..., \n",
      "None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda34\\envs\\gpu_env\\lib\\site-packages\\sklearn\\pipeline.py:230: UserWarning: Persisting input arguments took 2.77s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________fit_transform_one - 2.8s, 0.0min\n",
      "Processing DateTime Data\n",
      "Encoding....\n",
      "Size of the Dataset after processing (90000, 35074)\n",
      "SMOTE Over-sampling the minority class ...\n",
      "dataset has been added into \n",
      "Size of the Dateset after resampling (177300, 35074)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   2 out of  12 | elapsed:  1.6min remaining:  8.2min\n",
      "[Parallel(n_jobs=12)]: Done  12 out of  12 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "risk_estimator(behaviour='new', bootstrap=True, contamination=0.25,\n",
       "        max_features=0.6, max_samples=1000, n_estimators=1000, n_jobs=-1,\n",
       "        verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restimator = risk_estimator()\n",
    "restimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DateTime Data\n",
      "Encoding....\n"
     ]
    }
   ],
   "source": [
    "y_pred = restimator.predict(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9850\n",
      "           1       0.01      1.00      0.03       150\n",
      "\n",
      "   micro avg       0.01      0.01      0.01     10000\n",
      "   macro avg       0.01      0.50      0.01     10000\n",
      "weighted avg       0.00      0.01      0.00     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda34\\envs\\gpu_env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_holdout, y_pred, target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DateTime Data\n",
      "Encoding....\n"
     ]
    }
   ],
   "source": [
    "y_pred = restimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     88650\n",
      "           1       0.01      1.00      0.03      1350\n",
      "\n",
      "   micro avg       0.01      0.01      0.01     90000\n",
      "   macro avg       0.01      0.50      0.01     90000\n",
      "weighted avg       0.00      0.01      0.00     90000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda34\\envs\\gpu_env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, y_pred, target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"restimator.pkl\"\n",
    "modeling_cols = [c for c in ds.columns.tolist() if c not in identifiers+['anomalous_score', 'anomalous']]\n",
    "pickle.dump({'est': restimator, 'cols': modeling_cols}, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
