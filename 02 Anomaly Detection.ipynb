{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers = ['user_id', 'request_id', 'target_recipient_id']\n",
    "date_cols = ['date_user_created', 'date_request_submitted', 'date_request_received','date_request_transferred', \\\n",
    "             'first_attempt_date', 'first_success_date']\n",
    "categorical = ['addr_country_code', 'addr_city', 'recipient_country_code', 'flag_personal_business', 'payment_type', \\\n",
    "               'payment_status', 'ccy_send', 'ccy_target', 'transfer_to_self', 'sending_bank_name',  'sending_bank_country',\\\n",
    "               'payment_reference_classification', 'device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('AML_dataset.csv', parse_dates=date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['addr_city'] = ds['addr_city'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                                     object\n",
       "request_id                                  object\n",
       "target_recipient_id                         object\n",
       "date_user_created                   datetime64[ns]\n",
       "addr_country_code                           object\n",
       "addr_city                                   object\n",
       "recipient_country_code                      object\n",
       "flag_personal_business                      object\n",
       "payment_type                                object\n",
       "date_request_submitted              datetime64[ns]\n",
       "date_request_received               datetime64[ns]\n",
       "date_request_transferred            datetime64[ns]\n",
       "date_request_cancelled                      object\n",
       "invoice_value                              float64\n",
       "invoice_value_cancel                       float64\n",
       "flag_transferred                             int64\n",
       "payment_status                              object\n",
       "ccy_send                                    object\n",
       "ccy_target                                  object\n",
       "transfer_to_self                            object\n",
       "sending_bank_name                           object\n",
       "sending_bank_country                        object\n",
       "payment_reference_classification            object\n",
       "device                                      object\n",
       "transfer_sequence                          float64\n",
       "days_since_previous_req                    float64\n",
       "first_attempt_date                  datetime64[ns]\n",
       "first_success_date                  datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use scikit-learn implementation,\n",
    "For large dataset, we can use spark implementation\n",
    "##### https://github.com/titicaca/spark-iforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing all missing values\n",
    "\n",
    "Missing values should be investigted with SMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.externals.joblib import Memory\n",
    "\n",
    "class dateEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Processing DateTime Data')\n",
    "        print('Encoding....')\n",
    "        retX = pd.DataFrame()\n",
    "        #print(X)\n",
    "        for colname, col in X.iteritems():\n",
    "            retX[colname+'_dayofweek'] = col.dt.dayofweek\n",
    "            retX[colname+'_weekday'] = col.dt.weekday\n",
    "            retX[colname+'_hour'] = col.dt.hour\n",
    "            retX[colname+'_minute'] = col.dt.minute\n",
    "            retX[colname+'_day'] = col.dt.day\n",
    "            retX[colname+'_month'] = col.dt.month\n",
    "            #retX[colname+'_year'] = col.dt.year\n",
    "    \n",
    "        return retX\n",
    "    \n",
    "def prepare_pipeline(ds):\n",
    "    \n",
    "    numeric_features = ds.select_dtypes(include=[np.float or np.float]).columns.tolist()\n",
    "    categorical_features = ds.select_dtypes(include=['object']).columns.tolist()\n",
    "    date_features = ds.select_dtypes(include=[np.datetime64]).columns.tolist()\n",
    "    \n",
    "    cachedir = mkdtemp()\n",
    "    memory = Memory(cachedir=cachedir, verbose=1)\n",
    "\n",
    "    date_transformer = Pipeline(memory=memory, steps=[('dateEncoder', dateEncoder()), ('imputer', SimpleImputer(strategy='median', verbose=1))])\n",
    "    numeric_transformer = Pipeline(memory=memory, steps=[('imputer', SimpleImputer(strategy='median', verbose=1))])\n",
    "    categorical_transformer = Pipeline(memory=memory, steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing', verbose=1)),\n",
    "                                                             ('onehot', OneHotEncoder(handle_unknown='ignore'))])    \n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                                   ('dates', date_transformer, date_features),\n",
    "                                                   ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    Anomaly_Detector = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                       ('iforce', IsolationForest(n_estimators=1000, max_samples=1000,\\\n",
    "                                                                  max_features=0.6, contamination=0.25, n_jobs=-1, behaviour='new', verbose=1))])\n",
    "    \n",
    "    return Anomaly_Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda34\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:40: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='C:\\\\Users\\\\KHALID~1\\\\AppData\\\\Local\\\\Temp\\\\tmpyy611tct'\", use \"location='C:\\\\Users\\\\KHALID~1\\\\AppData\\\\Local\\\\Temp\\\\tmpyy611tct'\" instead.\n"
     ]
    }
   ],
   "source": [
    "Anomaly_Detector = prepare_pipeline(ds[[c for c in ds.columns.tolist() if c not in identifiers]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(dateEncoder(),         date_user_created date_request_submitted date_request_received  \\\n",
      "0     2014-01-27 15:02:00    2016-08-26 07:35:00   2016-01-09 08:31:00   \n",
      "1     2015-12-10 15:35:00    2016-10-23 22:54:00                   NaT   \n",
      "2     2016-04-10 11:42:00    2016-10-26 13:42:00   2016-10-26 14:06:00   \n",
      "3     2014-10-17 00:27:00    2015-01-28 23:36:00   2015-01-28 23:36:00   \n",
      "4     2015-12-08 07:45:00    2015-08-18 08:55:00   2015-08-18 09:12:00   \n",
      "5     2016-04-19 16:30:00    2016-02-10 11:59:00   2016-02-10 12:00:00   \n",
      "6     2015-10-15 19:48:00    2016-12-08 20:54:00   2016-12-08 20:54:00   \n",
      "7     2015-12-03 21:04:00    2015-05-31 09:49:00   2015-05-31 09:52:00   \n",
      "8     2015-04-30 16:05:00    2015-..., \n",
      "None, None)\n",
      "Processing DateTime Data\n",
      "Encoding....\n",
      "________________________________________________fit_transform_one - 0.2s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
      "       strategy='constant', verbose=1), \n",
      "      addr_country_code          addr_city recipient_country_code  \\\n",
      "0                   DEU             BERLIN                     GB   \n",
      "1                   CAN            TORONTO                     US   \n",
      "2                   GBR             BOLTON                     PT   \n",
      "3                   GBR            SWINDON                     IN   \n",
      "4                   FRA              PARIS                     GB   \n",
      "5                   GBR          LIVERPOOL                     BG   \n",
      "6                   USA             CORONA                     SE   \n",
      "7                   GBR             LONDON                     PL   \n",
      "8                   GBR             LONDON                     HU   \n",
      "9         ..., \n",
      "None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda34\\envs\\gpu_env\\lib\\site-packages\\sklearn\\pipeline.py:230: UserWarning: Persisting input arguments took 3.04s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________fit_transform_one - 3.0s, 0.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   2 out of  12 | elapsed:   41.8s remaining:  3.5min\n",
      "[Parallel(n_jobs=12)]: Done  12 out of  12 | elapsed:   43.3s finished\n"
     ]
    }
   ],
   "source": [
    "Anomaly_Detector = Anomaly_Detector.fit(ds[[c for c in ds.columns.tolist() if c not in identifiers]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DateTime Data\n",
      "Encoding....\n"
     ]
    }
   ],
   "source": [
    "ds['anomalous_score'] = Anomaly_Detector.decision_function(ds[[c for c in ds.columns.tolist() if c not in identifiers]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "outliers_fraction = 0.015\n",
    "threshold = stats.scoreatpercentile(ds['anomalous_score'], 100 * outliers_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005248514906945759"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['anomalous'] = ds['anomalous_score'].apply(lambda x: 0 if x>=threshold else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalous Transfers count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[ds['anomalous']==1]['anomalous'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Anomalous Transfer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[ds['anomalous']==0]['anomalous'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sensitiveity of the Anomaly Detector can be validated by SMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"ds.pkl\"\n",
    "pickle.dump(ds, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
